# Image-Captioning-Computer-Vision-and-NLP-
Image Recognition Semantic Alignments for Image Captioning using Computer Vision and Natural Language Processing.


Introduction
Reinforcement learning has become a standard way of training artificial agents that interact with an environment. Several works explored the idea of incorporating humans in the learning process, in order to help the reinforcement learning agent to learn faster and accurately. In most cases, a human teacher observes the agent act in an environment and can give additional guidance to the learner. We aim to exploit natural language processing to guide an RL agent. While this is possible in limited domains, it can hardly scale to the real scenarios with large action spaces requiring versatile language feedback. Here our goal is to allow a non-expert human teacher to give feedback to an RL agent in the form of natural language. In order to overcome this challenge a method called image captioning has been evolved with many advanced technologies which helps us to identify the image and generate description based on the feedback given by the humans and also binding the descriptions which are fed manually to the model during the training stage. 
Research plays a significant role in gathering information and this always helps to understand the better version of any concept. Research is done on what algorithms to choose for implementing the object detection, instance segmentation and localization of the object. Our team will be proceeding with research throughout the project as there are many new aspects to know and work with being a naïve for these datasets and algorithms. 

Related Work
Several works incorporate human feedback to help an RL agent learn faster. A few attempts have been made to advise an RL agent using language pioneering work translated advice to a short program which was then implemented as a neural network. The units in this network represent Boolean concepts, which recognize whether the observed state satisfies the constraints given by the program. Several recent approaches trained the captioning model with policy gradients in order to directly optimize for the desired performance metrics. Our work differs from the recent efforts in conversation modelling or visual dialog using Reinforcement Learning. There are several restrictions that can make methods unsuitable for image detection, such as computational constraints that impede scalability. While several captioning methods exist, we design our own which is phrase-based, allowing for natural guidance by a nonexpert. In order to overcome the difficulties we are focusing on achieving more accurate edge detection from a depth image and then modify the process using morphological operations. This model will yield more accurate edge detection from a depth image and will modify the process using morphological operations. In image recognition, various methods such as R-CNN, SPP-Net, Fast R-CNN and Faster R-CNN are used. These algorithms faced some problems due to processing time as it takes huge amount of time to train the model and real time image processing cannot be done. 

Captioning represents a natural way of showing that the algorithm understands a photograph to a non-expert observer and due to its significance, this domain has received significant attention achieving the impressive performance on standard benchmarks. There are different models which aim at image captioning but they lack in linguistic information and also focusing only on particular metrics rather all of them. 


Objectives
The main objective of this project is that to train the model with the unstructured data with the image dataset we have. And then caption the images using the natural language processing techniques and detect the objects within the image using the computer vision. Putting all the objectives and goals of this project together 
Generating textual descriptions for the images and then combine the breakthroughs from computer vision and natural language processing.
Implementing a model which classifies the image and then describe a image which involves the feature of generating the textual descriptions of the contents of the image.
Generating the textual descriptions for specific regions of the images in the dataset – annotating the images.
Researching and exploring on different algorithms and ways to fulfil and furnish the goals we are aiming at.
Implementing a neural network models for captioning through feature extraction and language modelling techniques. 
Create visualizations for the results we retrieved where the image along with a caption is depicted by the model. 
The above are all the objectives which are aimed for our project. 

